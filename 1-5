PROGRAM 1A:
import numpy as np 
def initialize_student_data(num_students):
    student_data = [] 
    for i in range(num_students):
        name = input(f"Get the student's name for Student {i+1}: ") 
        age = int(input(f"Get the age for {name}: ")) 
        math_score = float(input(f"Get the Math score for {name}: "))
        science_score = float(input(f"Get the Science score for {name}: "))
        physics_score = float(input(f"Get the Physics score for {name}: ")) 
        chemistry_score = float(input(f"Get the Chemistry score for {name}: "))
        student_data.append([name, age, math_score, science_score, physics_score, chemistry_score])
        student_data = np.array(student_data) 
        return student_data 
def calculate_overall_average(student_data):
    scores = student_data[:, 2:].astype(float) 
    overall_avg = np.mean(scores) 
    return overall_avg 

def top_students_overall(student_data, n):
    scores = student_data[:, 2:].astype(float) 
    overall_avg_scores = np.mean(scores, axis=1)
    top_indices = np.argsort(overall_avg_scores)[::-1][:n]
    top_students = student_data[top_indices] 
    return top_students 
def filter_students(student_data, min_age, min_score, subject='Math'):
    subject_index = {'Math': 2, 'Science': 3, 'Physics': 4, 'Chemistry': 5}[subject]
    filtered_students = student_data[(student_data[:, 1].astype(int) >= min_age) & (student_data[:, subject_index].astype(float) >= min_score)] 
    return filtered_students 
num_students = int(input("Get the number of students: ")) 
student_data = initialize_student_data(num_students)
print("\nInitial Student Data:")
print(student_data) 
print() 
overall_avg = calculate_overall_average(student_data) 
print(f"Overall Average Score of Students: {overall_avg:.2f}")
print() 
top_n = int(input("Get the number of top students to display: ")) 
top_students = top_students_overall(student_data, top_n) 
print(f"\nTop {top_n} Students based on Overall Average Score:") 
print(top_students) 
print() 
min_age_filter = int(input("Get the minimum age to filter students: ")) 
min_score_filter = float(input("Get the minimum score in Physics to filter students: ")) 
filtered_students = filter_students(student_data, min_age_filter, min_score_filter, subject='Physics')
print(f"\nStudents aged {min_age_filter} or older with at least {min_score_filter} in Physics:")
print(filtered_students) # Print filtered students
min_score_filter_chem = float(input("Get the minimum score in Chemistry to filter students: "))
filtered_students_chem = filter_students(student_data, min_age_filter, min_score_filter_chem, 
subject='Chemistry') # Filter students based on criteria
print(f"\nStudents aged {min_age_filter} or older with at least {min_score_filter_chem} in Chemistry:")
print(filtered_students_chem)

PROGRAM 1B:
# Import necessary libraries and modules
from sklearn.datasets import load_iris
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

iris = load_iris()
X = iris.data 
y = iris.target
df = pd.DataFrame(data=X, columns=iris.feature_names) 
df['target'] = y 
missing_values = df.isnull().sum()
print("Missing Values =", missing_values) 
summary_stats = df.describe() 
print(summary_stats) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) 

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train) 
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred) 
print(f'Accuracy: {accuracy}')

conf_matrix = confusion_matrix(y_test, y_pred) 
print(conf_matrix)

PROGRAM 2A:
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer = load_breast_cancer()

data = pd.DataFrame(cancer.data, columns=cancer.feature_names)
data['target'] = cancer.target

print(data.info())
print(data.head(1))
print(data.describe())
print(data.isnull().sum())
plt.figure(figsize = (10,6))
plt.plot(data.index, data['mean radius'], label = 'Mean Radius')
plt.title('Line Plot of Mean Radius')
plt.xlabel('Index')
plt.ylabel('Mean Radius')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize = (10,6))
plt.scatter(data['mean radius'], data['mean texture'], c = data['target'], cmap = 'coolwarm', alpha = 0.5)
plt.title('Scatter Plot of Mean Radius vs Mean Texture')
plt.xlabel('Mean Radius')
plt.ylabel('Mean Texture')
plt.grid(True)
plt.show()

plt.figure(figsize = (10,6))
plt.bar(data['target'].value_counts().index, data['target'].value_counts().values)
plt.title('Bar Plot of Target Calss Distribution')
plt.xlabel('Target Class')
plt.ylabel('Count')
plt.xticks([0, 1], ['Malignant', 'Benign'])
plt.grid(True)
plt.show()

plt.figure(figsize = (10,6))
plt.hist(data['mean area'], bins = 30, alpha=0.7)
plt.title('Histogram of Mean Area')
plt.xlabel('Mean Area')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

plt.figure(figsize = (10,6))
plt.boxplot([data[data['target'] == 0]['mean radius'], data[data['target']== 1]['mean radius']], labels = ['Maglignant', 'Benign'])
plt.title('Bar Plot of Mean Radius by Target Class')
plt.xlabel('Target Class')
plt.ylabel('Mean Radius')
plt.grid(True)
plt.show()

PROGRAM 2B:
import seaborn as sns
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer 

cancer = load_breast_cancer()
data = pd.DataFrame(cancer.data, columns=cancer.feature_names)
data['target']=cancer.target

print(data.info())
print(data.head(1))
print(data.describe())
print(data.isnull().sum()) 

plt.figure(figsize=(6, 4)) 
sns.countplot(x='target',data=data,palette='coolwarm') 
plt.title('Count Plot of Target Classes') 
plt.xlabel('Target Class') 
plt.ylabel('Count')
plt.xticks([0, 1],['Malignant', 'Benign'])
plt.show() 
plt.figure(figsize=(10, 6)) 
sns.kdeplot(data=data[data['target'] == 0]['mean radius'], shade=True, label='Malignant', color='r') 
sns.kdeplot(data=data[data['target'] == 1]['mean radius'], shade=True, label='Benign', color='b') 
plt.title('KDE Plot of Mean Radius') 
plt.xlabel('Mean Radius') 
plt.ylabel('Density') 
plt.legend() 
plt.show() 
plt.figure(figsize=(10, 6)) 
sns.violinplot(x='target', y='mean radius', data=data, palette='coolwarm') 
plt.title('Violin Plot of Mean Radius by Target Class')
plt.xlabel('Target Class') 
plt.ylabel('Mean Radius') 
plt.xticks([0, 1],['Malignant', 'Benign']) 
plt.show() 
sns.pairplot(data, vars=['mean radius', 'mean texture', 'mean perimeter', 'mean area'], 
hue='target', palette='coolwarm') 
plt.title('Pair Plot') 
plt.show() 
plt.figure(figsize=(20, 20)) 
sns.heatmap(data.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Heatmap') 
plt.show()

PROGRAM 3:
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names

print("Range of values before scaling:")
for i, feature_name in enumerate(feature_names):
    print(f"{feature_name}: {X[:, i].min()} to {X[:, i].max()}")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nRange of values after Scaling")
for i, feature_name in enumerate(feature_names[:X_train_scaled.shape[1]]):
    print(f"{feature_name}: {X_train_scaled[:, i].min():.2f} to {X_train_scaled[:, i].max():.2f}")

print("\nOriginal Training Data (first 3 rows):")
print(pd.DataFrame(X_train, columns=feature_names).head(3))

lda = LDA(n_components=2)
X_train_lda = lda.fit_transform(X_train_scaled, y_train)
X_test_lda = lda.transform(X_test_scaled)

print("\nTraining Data after LDA (first 3 rows):")
print(pd.DataFrame(X_train_lda, columns=['LDA component 1', 'LDA component 2']).head(3))

print("\nExplained Variance ratio:", lda.explained_variance_ratio_)
print("\nDimensions of the original dataset:", X_train.shape)
print("\nDimensions of the dataset after LDA:", X_train_lda.shape)

knn_original = KNeighborsClassifier(n_neighbors=3)
knn_original.fit(X_train_scaled, y_train)
y_pred_original = knn_original.predict(X_test_scaled)
accuracy_original = accuracy_score(y_test, y_pred_original)
conf_matrix_original = confusion_matrix(y_test, y_pred_original)

print("\nKNN Classifier on Original 4D Features:")
print(f"Accuracy: {accuracy_original:.2f}")
print("Confusion Matrix:\n", conf_matrix_original)

knn_lda = KNeighborsClassifier(n_neighbors=3)
knn_lda.fit(X_train_lda, y_train)
y_pred_lda = knn_lda.predict(X_test_lda)
accuracy_lda = accuracy_score(y_test, y_pred_lda)
conf_matrix_lda = confusion_matrix(y_test, y_pred_lda)

print("\nKNN Classifier on 2D LDA Features:")
print(f"Accuracy: {accuracy_lda:.2f}")
print("Confusion Matrix:\n", conf_matrix_lda)

PROGRAM 4:
import numpy as np # Import the numpy library for numerical operations
from sklearn.datasets import load_iris Import the load_iris function from scikit-learn datasets
from sklearn.preprocessing import StandardScaler Import StandardScaler for data standardization
from sklearn.decomposition import PCA #Import PCA for principal component analysis
from sklearn.cluster import KMeans # Import KMeans for k-means clustering
#Step 1: Load the Iris dataset
iris load_iris() # Load the Iris dataset into the variable iris
Xiris.data #Extract the features from the Iris dataset
#Step 2: Standardize the data
scaler-StandardScaler() # Create a StandardScaler object for data standardization
X scaled scaler.fit transform(X) # Standardize the features and store them in X scaled
#Step 3: Perform PCA
pcaPCA(n_components-2) # Create a PCA object to reduce the data to 2 principal components
X pea pea.fit transform(X scaled) #Apply PCA on the standardized data and store the transformed data in X pea
#Step 4: Print principal component details
print("Principal Component Details:") # Print the header for principal component details
print("\nExplained Variance Ratio:", pca.explained variance ratio) # Print the explained variance ratio of each principal component
print("\nPrincipal Components:") # Print the header for principal components
print(pca.components) # Print the principal components
#Step 5: Apply K-means clustering on PCA-reduced data
kmeans KMeans(n clusters-3, random state 42, n init-10) #Create a KMeans object with 3 clusters,
random state for reproducibility, and 10 initializations
kmeans.fit(X pea) # Fit the KMeans algorithm on the PCA-reduced data
y kmeans kmeans.predict(X pca) # Predict the cluster for each data point and store the cluster labels in y kmeans
#Step 6: Print cluster centers and cluster sizes
print("\nCluster Centers (in PCA-reduced space):") # Print the header for cluster centers
for i, center in enumerate(kmeans.cluster centers): # Iterate over the cluster centers
print(f"Cluster [i+1}: {center}") # Print the center of each cluster
print("\nCluster Sizes:") # Print the header for cluster sizes
for i, size in enumerate(np.bincount(y_kmeans)): # Iterate over the sizes of each cluster
print(f"Cluster (i+1): (size)") # Print the size of each cluster

PROGRAM 5:
import pandas as pd # Import the pandas library for data manipulation
from sklearn.model selection import train test split Import function to split data into training and testing sets
from sklearn.preprocessing import StandardScaler # Import StandardScaler to standardize features
from sklearn.xvm import SVC #Import Support Vector Classifier for SVM modeling
from sklearn, metrics import confusion_matrix, accuracy_score, precision_score, rocall_score, fl score Import various metrics to evaluate the model
#Lead the dataset
datupd.read_csv('winequality_red.csv') # Read the dataset from a CSV file into a DataFrame
#Display the details of the dataset
print(data.info()) # Print summary information about the DataFrame, including the data types and non-null counts of each column
#Features and target variable
X-data.drop('quality', axis-1) #Drop the 'quality' column to create the feature set (X) which contains all columns except 'quality'
y=data['quality #Create the target variable (y) which contains only the 'quality' column
#Preprocessing: Standardize the features
scaler StandardScaler() #Create an instance of Standard Scaler
X scaled scaler fit transform(X) Fit the scaler to the feature data and transform it, standardizing the features
#Split the data into training and testing sets
X_train, X_test, y_train, y_test-train_test_split(X_scaled, y, test size-0.3, random state-42)
model=SVC(kernel-rbf, gamma scale, C-1.0, class weight-balanced, probability True) # Create an instance of SVC with radial basis function kernel, automatic gamma scaling, regularization parameter C set to 1.0, balanced class weights, and probability estimates enabled
model.fit(X train, y train) # Train the SVM model using the training data
#Make predictions on the test set
y pred model.predict(X_test) # Use the trained model to predict the labels for the test set
Calculate and print accuracy
accuracy accuracy_score(y_test, y pred) #Calculate the accuracy of the predictions
print(finAccuracy: (accuracy: 4f)") # Print the accuracy with 4 decimal places
print("\nConfusion Matrix:")
print(cm) Print the confusion matrix
precision precision_score(y_test, y pred, average None) # Calculate precision scores for each class, without averaging
recall recall scorely test, y pred, average None) Calculate recall scores for each class, without averaging
fl=fl_score(y test, y pred, average=None) # Calculate FI scores for cach class, without averaging
print("\nPrecision for cach class:")
for i, p in enumerate(precision); # Iterate through each class's precision score
print(f"Class (i): (p.4f)") # Print the precision score for cach class with 4 decimal places
print("aRecall for each class:")
for i, r in enumerate(recall): # Iterate through each class's recall score
print(f"Class (i): (c.4f)") #Print the recall score for each class with 4 decimal places
print("aF1 Score for each class:")
for i, f' in enumerate(): # Iterate through each class's F1 score
print(f"Class (i): (f4f)") # Print the F1 score for each class with 4 decimal places
